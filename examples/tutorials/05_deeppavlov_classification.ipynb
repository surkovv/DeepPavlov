{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also run the notebook in [COLAB](https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/tutorials/05_deeppavlov_classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on DeepPavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "Intent recognition on SNIPS dataset: https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines that has already been recomposed to `csv` format and can be downloaded from http://files.deeppavlov.ai/datasets/snips_intents/train.csv\n",
    "\n",
    "FastText English word embeddings ~8Gb: http://files.deeppavlov.ai/deeppavlov_data/embeddings/wiki.en.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of the notebook with documentation links:\n",
    "\n",
    "1. [Data aggregation](#Data-aggregation)\n",
    "     * [DatasetReader](#DatasetReader): [docs link](https://deeppavlov.readthedocs.io/en/latest/apiref/dataset_readers.html)\n",
    "     * [DatasetIterator](#DatasetIterator): [docs link](https://deeppavlov.readthedocs.io/en/latest/apiref/dataset_iterators.html)\n",
    "2. [Data preprocessing](#Data-preprocessing): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)\n",
    "     * [Lowercasing](#Lowercasing)\n",
    "     * [Tokenization](#Tokenization)\n",
    "     * [Vocabulary](#Vocabulary)\n",
    "3. [Featurization](#Featurization): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html), [pre-trained embeddings link](https://deeppavlov.readthedocs.io/en/latest/intro/pretrained_vectors.html)\n",
    "    * [Bag-of-words embedder](#Bag-of-words)\n",
    "    * [TF-IDF vectorizer](#TF-IDF Vectorizer)\n",
    "    * [GloVe embedder](#GloVe-embedder)\n",
    "    * [Mean GloVe embedder](#Mean-GloVe-embedder)\n",
    "    * [GloVe weighted by TF-IDF embedder](#GloVe-weighted-by-TF-IDF-embedder)\n",
    "4. [Models](#Models): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/classifiers.html)\n",
    "    * [Building models in python](#Models-in-python)\n",
    "        - [Sklearn component classifiers](#SklearnComponent-classifier-on-Tfidf-features-in-python)\n",
    "        - [Keras classification model on GloVe emb](#KerasClassificationModel-on-GloVe-embeddings-in-python)\n",
    "        - [Sklearn component classifier on GloVe weighted emb](#SklearnComponent-classifier-on-GloVe-weighted-by-TF-IDF-embeddings-in-python)\n",
    "    * [Building models from configs](#Models-from-configs)\n",
    "        - [Sklearn component classifiers](#SklearnComponent-classifier-on-Tfidf-features-from-config)\n",
    "        - [Keras classification model](#KerasClassificationModel-on-fastText-embeddings-from-config)\n",
    "        - [Sklearn component classifier on GloVe weighted emb](#SklearnComponent-classifier-on-GloVe-weighted-by-TF-IDF-embeddings-from-config)\n",
    "    * [Bonus: pre-trained CNN model in DeepPavlov](#Bonus:-pre-trained-CNN-model-in-DeepPavlov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's download and look into data we will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:33:35.510 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-11-09 16:33:35.514 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /datasets/snips_intents/train.csv HTTP/1.1\" 200 980824\n",
      "2018-11-09 16:33:35.515 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/datasets/snips_intents/train.csv to snips/train.csv\n",
      "100%|██████████| 981k/981k [00:00<00:00, 22.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.utils import simple_download\n",
    "\n",
    "#download train data file for SNIPS\n",
    "simple_download(url=\"http://files.deeppavlov.ai/datasets/snips_intents/train.csv\", \n",
    "                destination=\"./snips/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,intents\r\n",
      "Add another song to the Cita RomГЎntica playlist. ,AddToPlaylist\r\n",
      "add clem burke in my playlist Pre-Party R&B Jams,AddToPlaylist\r\n",
      "Add Live from Aragon Ballroom to Trapeo,AddToPlaylist\r\n",
      "add Unite and Win to my night out,AddToPlaylist\r\n",
      "Add track to my Digster Future Hits,AddToPlaylist\r\n",
      "add the piano bar to my Cindy Wilson,AddToPlaylist\r\n",
      "Add Spanish Harlem Incident to cleaning the house,AddToPlaylist\r\n",
      "add The Greyest of Blue Skies in Indie EspaГ±ol my playlist,AddToPlaylist\r\n",
      "Add the name kids in the street to the plylist New Indie Mix,AddToPlaylist\r\n",
      "add album radar latino,AddToPlaylist\r\n",
      "Add Tranquility to the Latin Pop Rising playlist. ,AddToPlaylist\r\n",
      "Add d flame to the Dcode2016 playlist.,AddToPlaylist\r\n",
      "Add album to my fairy tales,AddToPlaylist\r\n",
      "I need another artist in the New Indie Mix playlist. ,AddToPlaylist\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 15 snips/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetReader\n",
    "\n",
    "Read data using `BasicClassificationDatasetReader` из DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:24.856 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-11-09 16:34:24.857 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n"
     ]
    }
   ],
   "source": [
    "# read data from particular columns of `.csv` file\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path='./snips/',\n",
    "    train='train.csv',\n",
    "    x = 'text',\n",
    "    y = 'intents'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a ready train/valid/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 15884), ('valid', 0), ('test', 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check train/valid/test sizes\n",
    "[(k, len(dr[k])) for k in dr.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetIterator\n",
    "\n",
    "Use `BasicClassificationDatasetIterator` to split `train` on `train` and `valid` and to generate batches of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:30.398 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n"
     ]
    }
   ],
   "source": [
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2\n",
    "train_iterator = BasicClassificationDatasetIterator(\n",
    "    data=dr,\n",
    "    field_to_split='train',  # field that will be splitted\n",
    "    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n",
    "    split_proportions=[0.8, 0.2],  #proportions for splitting\n",
    "    split_seed=23,  # seed for splitting dataset\n",
    "    seed=42)  # seed for iteration over dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Is it freezing in Offerman, California?\n",
      "y: ['GetWeather']\n",
      "=================\n",
      "x: put this song in the playlist Trap Land\n",
      "y: ['AddToPlaylist']\n",
      "=================\n",
      "x: show me a textbook with a rating of 2 and a maximum rating of 6 that is current\n",
      "y: ['RateBook']\n",
      "=================\n",
      "x: Will the weather be okay in Northern Luzon Heroes Hill National Park 4 and a half months from now?\n",
      "y: ['GetWeather']\n",
      "=================\n",
      "x: Rate the current album a four\n",
      "y: ['RateBook']\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "# one can get train instances (or any other data type including `all`)\n",
    "x_train, y_train = train_iterator.get_instances(data_type='train')\n",
    "for x, y in list(zip(x_train, y_train))[:5]:\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using lowercasing and tokenization as data preparation. \n",
    "\n",
    "DeepPavlov also contains several other preprocessors and tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StrLower` lowercases texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.str_lower import StrLower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is it freezing in offerman, california?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lower = StrLower()\n",
    "str_lower(['Is it freezing in Offerman, California?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "`NLTKTokenizer` can split string to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Is', 'it', 'freezing', 'in', 'Offerman', ',', 'California', '?']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "tokenizer(['Is it freezing in Offerman, California?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess all `train` part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Now we are ready to use `vocab`. They are very usefull for:\n",
    "* extracting class labels and converting labels to indices and vice versa,\n",
    "* building of characters or tokens vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vocabulary to collect all appeared in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='./snips/classes.dict',\n",
    "    load_path='./snips/classes.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:36.222 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 89: [saving vocabulary to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/classes.dict]\n"
     ]
    }
   ],
   "source": [
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what classes the dataset contains and their indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GetWeather', 0),\n",
       " ('PlayMusic', 1),\n",
       " ('SearchScreeningEvent', 2),\n",
       " ('BookRestaurant', 3),\n",
       " ('RateBook', 4),\n",
       " ('SearchCreativeWork', 5),\n",
       " ('AddToPlaylist', 6)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classes_vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./snips/tokens.dict',\n",
    "    load_path='./snips/tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:38.172 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 89: [saving vocabulary to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/tokens.dict]\n"
     ]
    }
   ],
   "source": [
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4564"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in dictionary\n",
    "len(token_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6953),\n",
       " ('a', 3917),\n",
       " ('in', 3265),\n",
       " ('to', 3203),\n",
       " ('for', 2814),\n",
       " ('of', 2401),\n",
       " ('.', 2400),\n",
       " ('i', 2079),\n",
       " ('at', 1935),\n",
       " ('play', 1703)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 most common words and number of times their appeared\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 36, 244, 4, 1, 29, 996, 20]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?'])))\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is it freezing in <UNK>, california?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(token_vocab(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "\n",
    "This part contains several possible ways of featurization of text samples. One can chose any appropriate vectorizer/embedder according to available resources and given task.\n",
    "\n",
    "Bag-of-words (BoW) and TF-IDF vectorizers converts text samples to vectors (one vector per sample) while fastText, GloVe, fastText weighted by TF-IDF embedders either produce an embedding vector per token or an embedding vector per text sample (if `mean` set to True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "Matches a vector to each text sample indicating which words appeared in the given sample: text -> binary vector $v$: \\[0, 1, 0, 0, 0, 1, ..., ...1, 0, 1\\]. \n",
    "\n",
    "Dimensionality of vector $v$ is equal to vocabulary size.\n",
    "\n",
    "$v_i$ == 1, if word $i$ is in the text,\n",
    "\n",
    "$v_i$ == 0, else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deeppavlov.models.embedders.bow_embedder import BoWEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, ..., 0, 0, 0], dtype=int32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize bag-of-words embedder giving total number of tokens\n",
    "bow = BoWEmbedder(depth=token_vocab.len)\n",
    "# it assumes indexed tokenized samples\n",
    "bow(token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 8 tokens are in the vocabulary\n",
    "sum(bow(token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?']))))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "Matches a vector to each text sample: text -> vector $v$ from $R^N$ where $N$ is a vocabulary size.\n",
    "\n",
    "$TF-IDF(token, document) = TF(token, document) * IDF(token, document)$\n",
    "\n",
    "$TF$ is a term frequency:\n",
    "\n",
    "$TF(token, document) = \\frac{n_{token}}{\\sum_{k}n_k}.$\n",
    "\n",
    "$IDF$ is a inverse document frequency:\n",
    "\n",
    "$IDF(token, all\\_documents) = \\frac{Total\\ number\\ of\\ documents}{number\\ of\\ documents\\ where\\ token\\ appeared}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SklearnComponent` in DeepPavlov is a universal wrapper for any vecotirzer/estimator from `sklearn` package. The only requirement to specify component usage is following: model class and name of infer method should be passed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.sklearn import SklearnComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:54.330 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/tfidf_v0.pkl\n",
      "2018-11-09 16:34:54.331 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n"
     ]
    }
   ],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:55.565 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2018-11-09 16:34:55.686 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/tfidf_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit on textual train instances and save it\n",
    "tfidf.fit(str_lower(train_iterator.get_instances(data_type='train')[0]))\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10709 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(str_lower(['Is it freezing in Offerman, California?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10709"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in the TF-IDF vocabulary\n",
    "len(tfidf.model.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embedder\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:34:58.552 INFO in 'summarizer.preprocessing.cleaner'['textcleaner'] at line 37: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download GloVe embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:35:00.958 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-11-09 16:35:00.981 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /embeddings/glove.6B.100d.txt HTTP/1.1\" 200 None\n",
      "2018-11-09 16:35:00.982 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt to glove.6B.100d.txt\n",
      "347MB [00:11, 29.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "simple_download(url=\"http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt\", \n",
    "                destination=\"./glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:38:06.81 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/glove.6B.100d.txt`]\n",
      "2018-11-09 16:38:06.82 INFO in 'gensim.models.utils_any2vec'['utils_any2vec'] at line 170: loading projection weights from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/glove.6B.100d.txt\n",
      "2018-11-09 16:38:06.82 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': '/home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/glove.6B.100d.txt'}\n",
      "2018-11-09 16:38:06.83 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='/home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/glove.6B.100d.txt'>}\n",
      "2018-11-09 16:38:32.92 INFO in 'gensim.models.utils_any2vec'['utils_any2vec'] at line 232: loaded (400000, 100) matrix from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "embedder = GloVeEmbedder(load_path='./glove.6B.100d.txt',\n",
    "                         dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, (100,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x num_tokens x embedding_dim)\n",
    "embedded_batch = embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "len(embedded_batch), len(embedded_batch[0]), embedded_batch[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean GloVe embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedder returns a vector per token while we want to get a vector per text sample. Therefore, let's calculate mean vector of embeddings of tokens. \n",
    "For that we can either init `GloVeEmbedder` with `mean=True` parameter (`mean=false` by default), or pass `mean=true` while calling function (this way `mean` value is assigned only for this call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, (100,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x embedding_dim)\n",
    "embedded_batch = embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?'])), mean=True) \n",
    "len(embedded_batch), embedded_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe weighted by TF-IDF embedder\n",
    "\n",
    "One of the possible ways to combine TF-IDF vectorizer and any token embedder is to weigh token embeddings by TF-IDF coefficients (therefore, `mean` set to True is obligatory to obtain embeddings of interest while it still **by default** returns embeddings of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.embedders.tfidf_weighted_embedder import TfidfWeightedEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_embedder = TfidfWeightedEmbedder(\n",
    "    embedder=embedder,  # our GloVe embedder instance\n",
    "    tokenizer=tokenizer,  # our tokenizer instance\n",
    "    mean=True,  # to return one vector per sample\n",
    "    vectorizer=tfidf  # our TF-IDF vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, (100,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x  embedding_dim)\n",
    "embedded_batch = weighted_embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "len(embedded_batch), embedded_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.metrics.accuracy import sets_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SklearnComponent classifier on Tfidf-features in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:38:44.881 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/logreg_v0.pkl\n",
      "2018-11-09 16:38:44.882 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.linear_model:LogisticRegression from scratch\n"
     ]
    }
   ],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:38:46.393 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.linear_model:LogisticRegression\n",
      "2018-11-09 16:38:46.625 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = cls(tfidf(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: I need seating at Floating restaurant in Tennessee for a group of 9\n",
      "True label: ['BookRestaurant']\n",
      "Predicted label: BookRestaurant\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained result\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted label: {}\".format(y_valid_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982373308152345"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate sets accuracy (because each element is a list of labels)\n",
    "sets_accuracy(np.squeeze(y_valid), y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassificationModel on GloVe embeddings in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel\n",
    "from deeppavlov.models.preprocessors.one_hotter import OneHotter\n",
    "from deeppavlov.models.classifiers.proba2labels import Proba2Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:09:46.672 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 242: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2018-11-01 11:09:46.994 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 128)      38528       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 128)      64128       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 128)      89728       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 128)      512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 128)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 128)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          38500       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 233,555\n",
      "Trainable params: 232,573\n",
      "Non-trainable params: 982\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Intialize `KerasClassificationModel` that composes CNN shallow-and-wide network \n",
    "# (name here as`cnn_model`)\n",
    "cls = KerasClassificationModel(save_path=\"./cnn_model_v0\", \n",
    "                               load_path=\"./cnn_model_v0\", \n",
    "                               embedding_size=embedder.dim,\n",
    "                               n_classes=classes_vocab.len,\n",
    "                               model_name=\"cnn_model\",\n",
    "                               text_size=15, # number of tokens\n",
    "                               kernel_sizes_cnn=[3, 5, 7],\n",
    "                               filters_cnn=128,\n",
    "                               dense_size=100,\n",
    "                               optimizer=\"Adam\",\n",
    "                               learning_rate=0.1,\n",
    "                               learning_rate_decay=0.01,\n",
    "                               loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `KerasClassificationModel` assumes one-hotted distribution of classes per sample.\n",
    "# `OneHotter` converts indices to one-hot vectors representation.\n",
    "#  To obtain indices we can use our `classes_vocab` intialized and fitted above\n",
    "onehotter = OneHotter(depth=classes_vocab.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 10 epochs\n",
    "for ep in range(10):\n",
    "    for x, y in train_iterator.gen_batches(batch_size=64, \n",
    "                                           data_type=\"train\"):\n",
    "        x_embed = embedder(tokenizer(str_lower(x)))\n",
    "        y_onehot = onehotter(classes_vocab(y))\n",
    "        cls.train_on_batch(x_embed, y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:12:21.498 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v0_opt.json]\n"
     ]
    }
   ],
   "source": [
    "# Save model weights and parameters\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infering on validation data we get probability distribution on given data.\n",
    "y_valid_pred = cls(embedder(tokenizer(str_lower(x_valid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert probability distribution to labels, \n",
    "# we first need to convert probabilities to indices,\n",
    "# and then using vocabulary `classes_vocab` convert indices to labels.\n",
    "# \n",
    "# `Proba2Labels` converts probabilities to indices and supports three different modes:\n",
    "# if `max_proba` is true, returns indices of the highest probabilities\n",
    "# if `confident_threshold` is given, returns indices with probabiltiies higher than threshold\n",
    "# if `top_n` is given, returns `top_n` indices with highest probabilities\n",
    "prob2labels = Proba2Labels(max_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: I need seating at Floating restaurant in Tennessee for a group of 9\n",
      "True label: ['BookRestaurant']\n",
      "Predicted probability distribution: {'GetWeather': 0.00014258868759498, 'PlayMusic': 0.00028503243811428547, 'SearchScreeningEvent': 0.00023403449449688196, 'BookRestaurant': 0.9942096471786499, 'RateBook': 0.0005214287666603923, 'SearchCreativeWork': 0.0004547557036858052, 'AddToPlaylist': 0.0007166761788539588}\n",
      "Predicted label: ['BookRestaurant']\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained result\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted probability distribution: {}\".format(dict(zip(classes_vocab.keys(), \n",
    "                                                               y_valid_pred[0]))))\n",
    "print(\"Predicted label: {}\".format(classes_vocab(prob2labels(y_valid_pred))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9842618822788795"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate sets accuracy\n",
    "sets_accuracy(y_valid, classes_vocab(prob2labels(y_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SklearnComponent classifier on GloVe weighted by TF-IDF embeddings in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:12:26.170 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from logreg_v1.pkl\n",
      "2018-11-01 11:12:26.171 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.linear_model:LogisticRegression from scratch\n"
     ]
    }
   ],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v1.pkl',\n",
    "    load_path='./logreg_v1.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:12:50.390 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.linear_model:LogisticRegression\n",
      "2018-11-01 11:12:52.105 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to logreg_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(weighted_embedder(str_lower(tokenizer(x_train))), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = cls(weighted_embedder(str_lower(tokenizer(x_valid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: I need seating at Floating restaurant in Tennessee for a group of 9\n",
      "True label: ['BookRestaurant']\n",
      "Predicted label: BookRestaurant\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained result\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted label: {}\".format(y_valid_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9184765502045955"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate sets accuracy (because each element is a list of labels)\n",
    "sets_accuracy(np.squeeze(y_valid), y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's free our memory from embeddings and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.reset()\n",
    "cls.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models from configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "from deeppavlov.core.commands.train import train_evaluate_model_from_config, _test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SklearnComponent classifier on Tfidf-features from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_config = {\n",
    "  \"dataset_reader\": {\n",
    "    \"class_name\": \"basic_classification_reader\",\n",
    "    \"x\": \"text\",\n",
    "    \"y\": \"intents\",\n",
    "    \"data_path\": \"./snips\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"class_name\": \"basic_classification_iterator\",\n",
    "    \"seed\": 42,\n",
    "    \"split_seed\": 23,\n",
    "    \"field_to_split\": \"train\",\n",
    "    \"split_fields\": [\n",
    "      \"train\",\n",
    "      \"valid\"\n",
    "    ],\n",
    "    \"split_proportions\": [\n",
    "      0.9,\n",
    "      0.1\n",
    "    ]\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\n",
    "      \"x\"\n",
    "    ],\n",
    "    \"in_y\": [\n",
    "      \"y\"\n",
    "    ],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"id\": \"classes_vocab\",\n",
    "        \"class_name\": \"simple_vocab\",\n",
    "        \"fit_on\": [\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"save_path\": \"./snips/classes.dict\",\n",
    "        \"load_path\": \"./snips/classes.dict\",\n",
    "        \"in\": \"y\",\n",
    "        \"out\": \"y_ids\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"x_vec\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x\",\n",
    "          \"y_ids\"\n",
    "        ],\n",
    "        \"id\": \"tfidf_vec\",\n",
    "        \"class_name\": \"sklearn_component\",\n",
    "        \"save_path\": \"tfidf_v1.pkl\",\n",
    "        \"load_path\": \"tfidf_v1.pkl\",\n",
    "        \"model_class\": \"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "        \"infer_method\": \"transform\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x\",\n",
    "        \"out\": \"x_tok\",\n",
    "        \"id\": \"my_tokenizer\",\n",
    "        \"class_name\": \"nltk_moses_tokenizer\",\n",
    "        \"tokenizer\": \"wordpunct_tokenize\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x_vec\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"y_pred\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x_vec\",\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"class_name\": \"sklearn_component\",\n",
    "        \"main\": True,\n",
    "        \"save_path\": \"logreg_v2.pkl\",\n",
    "        \"load_path\": \"logreg_v2.pkl\",\n",
    "        \"model_class\": \"sklearn.linear_model:LogisticRegression\",\n",
    "        \"infer_method\": \"predict\",\n",
    "        \"ensure_list_output\": True\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\n",
    "      \"y_pred\"\n",
    "    ]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"batch_size\": 64,\n",
    "    \"metrics\": [\n",
    "      \"accuracy\"\n",
    "    ],\n",
    "    \"validate_best\": True,\n",
    "    \"test_best\": False\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:41:44.918 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/valid.csv file\n",
      "2018-11-09 16:41:44.918 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/test.csv file\n",
      "2018-11-09 16:41:44.919 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-11-09 16:41:44.924 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 100: [loading vocabulary from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/classes.dict]\n",
      "2018-11-09 16:41:44.950 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 89: [saving vocabulary to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/classes.dict]\n",
      "2018-11-09 16:41:44.951 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/tfidf_v1.pkl\n",
      "2018-11-09 16:41:44.952 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n",
      "2018-11-09 16:41:45.13 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2018-11-09 16:41:45.138 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/tfidf_v1.pkl\n",
      "2018-11-09 16:41:45.162 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/logreg_v2.pkl\n",
      "2018-11-09 16:41:45.162 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.linear_model:LogisticRegression from scratch\n",
      "2018-11-09 16:41:45.950 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.linear_model:LogisticRegression\n",
      "2018-11-09 16:41:46.240 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/logreg_v2.pkl\n",
      "2018-11-09 16:41:46.242 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 100: [loading vocabulary from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/classes.dict]\n",
      "2018-11-09 16:41:46.244 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/tfidf_v1.pkl\n",
      "2018-11-09 16:41:46.248 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-11-09 16:41:46.249 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-11-09 16:41:46.250 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/logreg_v2.pkl\n",
      "2018-11-09 16:41:46.251 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-11-09 16:41:46.252 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-11-09 16:41:46.253 INFO in 'deeppavlov.core.commands.train'['train'] at line 212: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"accuracy\": 0.983}, \"time_spent\": \"0:00:01\"}}\n"
     ]
    }
   ],
   "source": [
    "# we can train and evaluate model from config\n",
    "m = train_evaluate_model_from_config(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 16:41:48.877 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 100: [loading vocabulary from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/snips/classes.dict]\n",
      "2018-11-09 16:41:48.878 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/tfidf_v1.pkl\n",
      "2018-11-09 16:41:48.883 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-11-09 16:41:48.884 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-11-09 16:41:48.886 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/examples/tutorials/logreg_v2.pkl\n",
      "2018-11-09 16:41:48.888 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-11-09 16:41:48.888 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# or we can just load pre-trained model (conicides with what we did above)\n",
    "m = build_model(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassificationModel on GloVe embeddings from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_config = {\n",
    "  \"dataset_reader\": {\n",
    "    \"class_name\": \"basic_classification_reader\",\n",
    "    \"x\": \"text\",\n",
    "    \"y\": \"intents\",\n",
    "    \"data_path\": \"snips\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"class_name\": \"basic_classification_iterator\",\n",
    "    \"seed\": 42,\n",
    "    \"split_seed\": 23,\n",
    "    \"field_to_split\": \"train\",\n",
    "    \"split_fields\": [\n",
    "      \"train\",\n",
    "      \"valid\"\n",
    "    ],\n",
    "    \"split_proportions\": [\n",
    "      0.9,\n",
    "      0.1\n",
    "    ]\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\n",
    "      \"x\"\n",
    "    ],\n",
    "    \"in_y\": [\n",
    "      \"y\"\n",
    "    ],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"id\": \"classes_vocab\",\n",
    "        \"class_name\": \"simple_vocab\",\n",
    "        \"fit_on\": [\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"level\": \"token\",\n",
    "        \"save_path\": \"./snips/classes.dict\",\n",
    "        \"load_path\": \"./snips/classes.dict\",\n",
    "        \"in\": \"y\",\n",
    "        \"out\": \"y_ids\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x\",\n",
    "        \"out\": \"x_tok\",\n",
    "        \"id\": \"my_tokenizer\",\n",
    "        \"class_name\": \"nltk_tokenizer\",\n",
    "        \"tokenizer\": \"wordpunct_tokenize\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x_tok\",\n",
    "        \"out\": \"x_emb\",\n",
    "        \"id\": \"my_embedder\",\n",
    "        \"class_name\": \"glove\",\n",
    "        \"load_path\": \"./glove.6B.100d.txt\",\n",
    "        \"dim\": 100\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_ids\",\n",
    "        \"out\": \"y_onehot\",\n",
    "        \"class_name\": \"one_hotter\",\n",
    "        \"depth\": \"#classes_vocab.len\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x_emb\"\n",
    "        ],\n",
    "        \"in_y\": [\n",
    "          \"y_onehot\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"y_pred_probas\"\n",
    "        ],\n",
    "        \"main\": True,\n",
    "        \"class_name\": \"keras_classification_model\",\n",
    "        \"save_path\": \"./cnn_model_v1\",\n",
    "        \"load_path\": \"./cnn_model_v1\",\n",
    "        \"embedding_size\": \"#my_embedder.dim\",\n",
    "        \"n_classes\": \"#classes_vocab.len\",\n",
    "        \"kernel_sizes_cnn\": [\n",
    "          1,\n",
    "          2,\n",
    "          3\n",
    "        ],\n",
    "        \"filters_cnn\": 256,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"learning_rate_decay\": 0.1,\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"text_size\": 15,\n",
    "        \"coef_reg_cnn\": 1e-4,\n",
    "        \"coef_reg_den\": 1e-4,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"dense_size\": 100,\n",
    "        \"model_name\": \"cnn_model\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_pred_probas\",\n",
    "        \"out\": \"y_pred_ids\",\n",
    "        \"class_name\": \"proba2labels\",\n",
    "        \"max_proba\": True\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_pred_ids\",\n",
    "        \"out\": \"y_pred_labels\",\n",
    "        \"ref\": \"classes_vocab\"\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\n",
    "      \"y_pred_labels\"\n",
    "    ]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"metrics\": [\n",
    "      \"sets_accuracy\",\n",
    "      \"f1_macro\",\n",
    "      {\n",
    "        \"name\": \"roc_auc\",\n",
    "        \"inputs\": [\"y_onehot\", \"y_pred_probas\"]\n",
    "      }\n",
    "    ],\n",
    "    \"validation_patience\": 5,\n",
    "    \"val_every_n_epochs\": 1,\n",
    "    \"log_every_n_epochs\": 1,\n",
    "    \"show_examples\": True,\n",
    "    \"validate_best\": True,\n",
    "    \"test_best\": False\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:17:31.291 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-11-01 11:17:31.291 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n",
      "2018-11-01 11:17:31.292 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-11-01 11:17:31.297 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-11-01 11:17:31.302 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/classes.dict]\n",
      "2018-11-01 11:17:31.303 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `glove.6B.100d.txt`]\n",
      "2018-11-01 11:17:31.303 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from glove.6B.100d.txt\n",
      "2018-11-01 11:17:31.304 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': 'glove.6B.100d.txt'}\n",
      "2018-11-01 11:17:31.304 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='glove.6B.100d.txt'>}\n",
      "2018-11-01 11:17:52.872 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from glove.6B.100d.txt\n",
      "2018-11-01 11:17:52.889 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 242: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2018-11-01 11:17:53.262 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      25856       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      51456       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 235,475\n",
      "Trainable params: 233,725\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "/home/dilyara/anaconda3/envs/clean_deep36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-11-01 11:17:53.542 INFO in 'deeppavlov.core.commands.train'['train'] at line 360: New best sets_accuracy of 0.1517\n",
      "2018-11-01 11:17:53.543 INFO in 'deeppavlov.core.commands.train'['train'] at line 362: Saving model\n",
      "2018-11-01 11:17:53.543 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.1517, \"f1_macro\": 0.1107, \"roc_auc\": 0.5186}, \"time_spent\": \"0:00:01\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:17:57.713 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9327\n",
      "2018-11-01 11:17:57.714 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:17:57.714 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 1, \"batches_seen\": 224, \"train_examples_seen\": 14295, \"metrics\": {\"sets_accuracy\": 0.9027, \"f1_macro\": 0.9029, \"roc_auc\": 0.9797}, \"time_spent\": \"0:00:05\", \"examples\": [{\"x\": \"Add lisa m to my guitar hero live playlist\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"AddToPlaylist\"]}], \"loss\": 1.3867814327989305}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9327, \"f1_macro\": 0.9328, \"roc_auc\": 0.9959}, \"time_spent\": \"0:00:05\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 1, \"batches_seen\": 224, \"train_examples_seen\": 14295, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:00.79 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9478\n",
      "2018-11-01 11:18:00.80 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:00.80 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 2, \"batches_seen\": 448, \"train_examples_seen\": 28590, \"metrics\": {\"sets_accuracy\": 0.9543, \"f1_macro\": 0.9548, \"roc_auc\": 0.9974}, \"time_spent\": \"0:00:07\", \"examples\": [{\"x\": \"I give The Monkey and the Tiger a rating of 2 points.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"RateBook\"]}], \"loss\": 1.2632445618510246}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9478, \"f1_macro\": 0.9473, \"roc_auc\": 0.997}, \"time_spent\": \"0:00:07\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 2, \"batches_seen\": 448, \"train_examples_seen\": 28590, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:02.625 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9497\n",
      "2018-11-01 11:18:02.625 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:02.626 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 3, \"batches_seen\": 672, \"train_examples_seen\": 42885, \"metrics\": {\"sets_accuracy\": 0.9593, \"f1_macro\": 0.9596, \"roc_auc\": 0.9979}, \"time_spent\": \"0:00:10\", \"examples\": [{\"x\": \"play Iheart tunes by Neil Finn\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"PlayMusic\"]}], \"loss\": 1.2202769278415613}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9497, \"f1_macro\": 0.9492, \"roc_auc\": 0.9974}, \"time_spent\": \"0:00:10\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 3, \"batches_seen\": 672, \"train_examples_seen\": 42885, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:04.987 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9534\n",
      "2018-11-01 11:18:04.988 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:04.988 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 4, \"batches_seen\": 896, \"train_examples_seen\": 57180, \"metrics\": {\"sets_accuracy\": 0.9633, \"f1_macro\": 0.9636, \"roc_auc\": 0.9982}, \"time_spent\": \"0:00:12\", \"examples\": [{\"x\": \"Please play a song off the Curtis Lee album Rough Diamonds\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"PlayMusic\"]}], \"loss\": 1.1903010856892382}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9534, \"f1_macro\": 0.953, \"roc_auc\": 0.9976}, \"time_spent\": \"0:00:12\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 4, \"batches_seen\": 896, \"train_examples_seen\": 57180, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:07.329 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9553\n",
      "2018-11-01 11:18:07.329 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:07.330 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 5, \"batches_seen\": 1120, \"train_examples_seen\": 71475, \"metrics\": {\"sets_accuracy\": 0.9659, \"f1_macro\": 0.9662, \"roc_auc\": 0.9984}, \"time_spent\": \"0:00:14\", \"examples\": [{\"x\": \"Give me Slovakia's weather forecast for eight am\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"GetWeather\"]}], \"loss\": 1.1678305316184248}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9553, \"f1_macro\": 0.9548, \"roc_auc\": 0.9977}, \"time_spent\": \"0:00:15\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 5, \"batches_seen\": 1120, \"train_examples_seen\": 71475, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:09.857 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9578\n",
      "2018-11-01 11:18:09.857 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:09.858 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 6, \"batches_seen\": 1344, \"train_examples_seen\": 85770, \"metrics\": {\"sets_accuracy\": 0.9676, \"f1_macro\": 0.9678, \"roc_auc\": 0.9985}, \"time_spent\": \"0:00:17\", \"examples\": [{\"x\": \"rate this current textbook 0 points\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"RateBook\"]}], \"loss\": 1.1507995431976659}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9578, \"f1_macro\": 0.9574, \"roc_auc\": 0.9979}, \"time_spent\": \"0:00:17\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 6, \"batches_seen\": 1344, \"train_examples_seen\": 85770, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:12.237 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9585\n",
      "2018-11-01 11:18:12.238 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:12.238 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 7, \"batches_seen\": 1568, \"train_examples_seen\": 100065, \"metrics\": {\"sets_accuracy\": 0.9687, \"f1_macro\": 0.9688, \"roc_auc\": 0.9986}, \"time_spent\": \"0:00:19\", \"examples\": [{\"x\": \"I need a bar for four that serves argentinian in D'Iberville, WY for twelve PM\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"loss\": 1.137291977448123}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9585, \"f1_macro\": 0.958, \"roc_auc\": 0.998}, \"time_spent\": \"0:00:19\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 7, \"batches_seen\": 1568, \"train_examples_seen\": 100065, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:14.601 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9604\n",
      "2018-11-01 11:18:14.601 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:14.601 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 8, \"batches_seen\": 1792, \"train_examples_seen\": 114360, \"metrics\": {\"sets_accuracy\": 0.97, \"f1_macro\": 0.9702, \"roc_auc\": 0.9987}, \"time_spent\": \"0:00:22\", \"examples\": [{\"x\": \"play The Sea Cabinet\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"SearchCreativeWork\"]}], \"loss\": 1.1260945099805082}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9604, \"f1_macro\": 0.9599, \"roc_auc\": 0.9981}, \"time_spent\": \"0:00:22\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 8, \"batches_seen\": 1792, \"train_examples_seen\": 114360, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:17.115 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.961\n",
      "2018-11-01 11:18:17.116 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:17.116 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 9, \"batches_seen\": 2016, \"train_examples_seen\": 128655, \"metrics\": {\"sets_accuracy\": 0.97, \"f1_macro\": 0.9702, \"roc_auc\": 0.9987}, \"time_spent\": \"0:00:24\", \"examples\": [{\"x\": \"add Ik Tara to laundry playlst\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"AddToPlaylist\"]}], \"loss\": 1.1170877803649222}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.961, \"f1_macro\": 0.9605, \"roc_auc\": 0.9981}, \"time_spent\": \"0:00:24\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 9, \"batches_seen\": 2016, \"train_examples_seen\": 128655, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:19.616 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9622\n",
      "2018-11-01 11:18:19.617 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-11-01 11:18:19.617 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n",
      "2018-11-01 11:18:19.671 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-11-01 11:18:19.672 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `glove.6B.100d.txt`]\n",
      "2018-11-01 11:18:19.672 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from glove.6B.100d.txt\n",
      "2018-11-01 11:18:19.673 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': 'glove.6B.100d.txt'}\n",
      "2018-11-01 11:18:19.673 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='glove.6B.100d.txt'>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 10, \"batches_seen\": 2240, \"train_examples_seen\": 142950, \"metrics\": {\"sets_accuracy\": 0.9713, \"f1_macro\": 0.9715, \"roc_auc\": 0.9988}, \"time_spent\": \"0:00:27\", \"examples\": [{\"x\": \"Is it going to be hot in Karthaus at 7 AM?\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"GetWeather\"]}], \"loss\": 1.1077362325574671}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9622, \"f1_macro\": 0.9617, \"roc_auc\": 0.9982}, \"time_spent\": \"0:00:27\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 10, \"batches_seen\": 2240, \"train_examples_seen\": 142950, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:42.734 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from glove.6B.100d.txt\n",
      "2018-11-01 11:18:42.740 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-11-01 11:18:43.157 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from cnn_model_v1.h5]\n",
      "2018-11-01 11:18:43.409 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      25856       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      51456       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 235,475\n",
      "Trainable params: 233,725\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "2018-11-01 11:18:43.420 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9622, \"f1_macro\": 0.9617, \"roc_auc\": 0.9982}, \"time_spent\": \"0:00:01\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}]}}\n"
     ]
    }
   ],
   "source": [
    "# we can train and evaluate model from config\n",
    "m = train_evaluate_model_from_config(cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:18:43.729 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-11-01 11:18:43.731 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `glove.6B.100d.txt`]\n",
      "2018-11-01 11:18:43.731 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from glove.6B.100d.txt\n",
      "2018-11-01 11:18:43.732 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': 'glove.6B.100d.txt'}\n",
      "2018-11-01 11:18:43.733 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='glove.6B.100d.txt'>}\n",
      "2018-11-01 11:19:06.676 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from glove.6B.100d.txt\n",
      "2018-11-01 11:19:06.691 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-11-01 11:19:07.60 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from cnn_model_v1.h5]\n",
      "2018-11-01 11:19:07.257 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      25856       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      51456       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 235,475\n",
      "Trainable params: 233,725\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# or we can just load pre-trained model (conicides with what we did above)\n",
    "m = build_model(cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SklearnComponent classifier on GloVe weighted by TF-IDF embeddings from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_config = {\n",
    "  \"dataset_reader\": {\n",
    "    \"class_name\": \"basic_classification_reader\",\n",
    "    \"x\": \"text\",\n",
    "    \"y\": \"intents\",\n",
    "    \"data_path\": \"snips\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"class_name\": \"basic_classification_iterator\",\n",
    "    \"seed\": 42,\n",
    "      \"split_seed\": 23,\n",
    "    \"field_to_split\": \"train\",\n",
    "    \"split_fields\": [\n",
    "      \"train\",\n",
    "      \"valid\"\n",
    "    ],\n",
    "    \"split_proportions\": [\n",
    "      0.9,\n",
    "      0.1\n",
    "    ]\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\n",
    "      \"x\"\n",
    "    ],\n",
    "    \"in_y\": [\n",
    "      \"y\"\n",
    "    ],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"id\": \"classes_vocab\",\n",
    "        \"class_name\": \"simple_vocab\",\n",
    "        \"fit_on\": [\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"save_path\": \"./snips/classes.dict\",\n",
    "        \"load_path\": \"./snips/classes.dict\",\n",
    "        \"in\": \"y\",\n",
    "        \"out\": \"y_ids\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"x_vec\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x\",\n",
    "          \"y_ids\"\n",
    "        ],\n",
    "        \"id\": \"my_tfidf_vectorizer\",\n",
    "        \"class_name\": \"sklearn_component\",\n",
    "        \"save_path\": \"tfidf_v2.pkl\",\n",
    "        \"load_path\": \"tfidf_v2.pkl\",\n",
    "        \"model_class\": \"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "        \"infer_method\": \"transform\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x\",\n",
    "        \"out\": \"x_tok\",\n",
    "        \"id\": \"my_tokenizer\",\n",
    "        \"class_name\": \"nltk_moses_tokenizer\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x_tok\",\n",
    "        \"out\": \"x_emb\",\n",
    "        \"id\": \"my_embedder\",\n",
    "        \"class_name\": \"glove\",\n",
    "        \"save_path\": \"./glove.6B.100d.txt\",\n",
    "        \"load_path\": \"./glove.6B.100d.txt\",\n",
    "        \"dim\": 100\n",
    "      },\n",
    "      {\n",
    "        \"class_name\": \"one_hotter\",\n",
    "        \"id\": \"my_onehotter\",\n",
    "        \"depth\": \"#classes_vocab.len\",\n",
    "        \"in\": \"y_ids\",\n",
    "        \"out\": \"y_onehot\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x_tok\",\n",
    "        \"out\": \"x_weighted_emb\",\n",
    "        \"class_name\": \"tfidf_weighted\",\n",
    "        \"id\": \"my_weighted_embedder\",\n",
    "        \"embedder\": \"#my_embedder\",\n",
    "        \"tokenizer\": \"#my_tokenizer\",\n",
    "        \"vectorizer\": \"#my_tfidf_vectorizer\",\n",
    "          \"mean\": True\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x_weighted_emb\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"y_pred\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x_weighted_emb\",\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"class_name\": \"sklearn_component\",\n",
    "        \"main\": True,\n",
    "        \"save_path\": \"logreg_v3.pkl\",\n",
    "        \"load_path\": \"logreg_v3.pkl\",\n",
    "        \"model_class\": \"sklearn.linear_model:LogisticRegression\",\n",
    "        \"infer_method\": \"predict\",\n",
    "        \"ensure_list_output\": True\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\n",
    "      \"y_pred\"\n",
    "    ]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"metrics\": [\n",
    "      \"sets_accuracy\"\n",
    "    ],\n",
    "    \"show_examples\": False,\n",
    "    \"validate_best\": True,\n",
    "    \"test_best\": False\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:24:06.727 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-11-01 11:24:06.728 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n",
      "2018-11-01 11:24:06.729 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-11-01 11:24:06.732 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-11-01 11:24:06.737 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/classes.dict]\n",
      "2018-11-01 11:24:06.739 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from tfidf_v2.pkl\n",
      "2018-11-01 11:24:06.739 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n",
      "2018-11-01 11:24:06.763 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2018-11-01 11:24:06.870 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to tfidf_v2.pkl\n",
      "2018-11-01 11:24:06.888 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `glove.6B.100d.txt`]\n",
      "2018-11-01 11:24:06.888 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from glove.6B.100d.txt\n",
      "2018-11-01 11:24:06.888 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': 'glove.6B.100d.txt'}\n",
      "2018-11-01 11:24:06.889 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='glove.6B.100d.txt'>}\n",
      "2018-11-01 11:24:28.8 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from glove.6B.100d.txt\n",
      "2018-11-01 11:24:28.14 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from logreg_v3.pkl\n",
      "2018-11-01 11:24:28.15 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.linear_model:LogisticRegression from scratch\n",
      "2018-11-01 11:24:50.125 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.linear_model:LogisticRegression\n",
      "2018-11-01 11:24:52.545 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to logreg_v3.pkl\n",
      "2018-11-01 11:24:52.584 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-11-01 11:24:52.586 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v2.pkl\n",
      "2018-11-01 11:24:52.592 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-11-01 11:24:52.592 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-11-01 11:24:52.593 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `glove.6B.100d.txt`]\n",
      "2018-11-01 11:24:52.594 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from glove.6B.100d.txt\n",
      "2018-11-01 11:24:52.594 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': 'glove.6B.100d.txt'}\n",
      "2018-11-01 11:24:52.595 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='glove.6B.100d.txt'>}\n",
      "2018-11-01 11:25:14.802 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from glove.6B.100d.txt\n",
      "2018-11-01 11:25:14.809 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from logreg_v3.pkl\n",
      "2018-11-01 11:25:14.810 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-11-01 11:25:14.810 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-11-01 11:25:14.821 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9283}, \"time_spent\": \"0:00:03\"}}\n"
     ]
    }
   ],
   "source": [
    "# we can train and evaluate model from config\n",
    "m = train_evaluate_model_from_config(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 11:25:17.318 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-11-01 11:25:17.319 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v2.pkl\n",
      "2018-11-01 11:25:17.324 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-11-01 11:25:17.325 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-11-01 11:25:17.326 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `glove.6B.100d.txt`]\n",
      "2018-11-01 11:25:17.326 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from glove.6B.100d.txt\n",
      "2018-11-01 11:25:17.326 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 149: {'kw': {}, 'mode': 'rb', 'uri': 'glove.6B.100d.txt'}\n",
      "2018-11-01 11:25:17.327 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 621: encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='glove.6B.100d.txt'>}\n",
      "2018-11-01 11:25:39.24 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from glove.6B.100d.txt\n",
      "2018-11-01 11:25:39.38 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from logreg_v3.pkl\n",
      "2018-11-01 11:25:39.39 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-11-01 11:25:39.40 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# or we can just load pre-trained model (conicides with what we did above)\n",
    "m = build_model(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's free memory\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: pre-trained CNN model in DeepPavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model files (`wiki.en.bin` 8Gb embeddings):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! python -m deeppavlov download intents_snips_big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate metrics on validation set (no test set provided):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! python -m deeppavlov evaluate intents_snips_big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or one can use model from python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import deeppavlov\n",
    "from deeppavlov import build_model\n",
    "from deeppavlov.download import deep_download\n",
    "\n",
    "config_path = Path(deeppavlov.__file__).parent.joinpath('configs/classifiers/intents_snips_big.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-31 17:10:28.776 INFO in 'deeppavlov.download'['download'] at line 112: Downloading...\n",
      "2018-10-31 17:10:28.778 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-31 17:10:28.801 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /datasets/snips_intents/train.csv HTTP/1.1\" 200 980824\n",
      "2018-10-31 17:10:28.802 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/datasets/snips_intents/train.csv to /home/dilyara/Documents/GitHub/DeepPavlov/download/snips/train.csv\n",
      "100%|██████████| 981k/981k [00:00<00:00, 13.5MB/s]\n",
      "2018-10-31 17:10:28.880 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-31 17:10:28.908 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/classifiers/intents_snips_v8.tar.gz HTTP/1.1\" 200 2018386\n",
      "2018-10-31 17:10:28.910 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/deeppavlov_data/classifiers/intents_snips_v8.tar.gz to /home/dilyara/Documents/GitHub/DeepPavlov/download/intents_snips_v8.tar.gz\n",
      "100%|██████████| 2.02M/2.02M [00:00<00:00, 23.4MB/s]\n",
      "2018-10-31 17:10:28.998 INFO in 'deeppavlov.core.data.utils'['utils'] at line 197: Extracting /home/dilyara/Documents/GitHub/DeepPavlov/download/intents_snips_v8.tar.gz archive into /home/dilyara/Documents/GitHub/DeepPavlov/download/classifiers\n",
      "2018-10-31 17:10:29.19 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-31 17:10:29.61 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/embeddings/wiki.en.bin HTTP/1.1\" 200 8493673445\n",
      "2018-10-31 17:10:29.62 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/deeppavlov_data/embeddings/wiki.en.bin to /home/dilyara/Documents/GitHub/DeepPavlov/download/embeddings/wiki.en.bin\n",
      "100%|██████████| 8.49G/8.49G [02:22<00:00, 59.7MB/s]\n",
      "2018-10-31 17:12:51.285 INFO in 'deeppavlov.download'['download'] at line 114: \n",
      "Download successful!\n"
     ]
    }
   ],
   "source": [
    "# let's download all the required data - model files, embeddings, vocabularies\n",
    "deep_download(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-31 17:12:51.320 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from /home/dilyara/Documents/GitHub/DeepPavlov/download/classifiers/intents_snips_v8/classes.dict]\n",
      "2018-10-31 17:12:51.656 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `/home/dilyara/Documents/GitHub/DeepPavlov/download/embeddings/wiki.en.bin`]\n",
      "2018-10-31 17:13:13.599 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-31 17:13:14.75 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from model.h5]\n",
      "2018-10-31 17:13:14.309 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# now one can initialize model\n",
    "m = build_model(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's free memory\n",
    "del m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-31 17:15:59.728 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find /home/dilyara/Documents/GitHub/DeepPavlov/download/snips/valid.csv file\n",
      "2018-10-31 17:15:59.729 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find /home/dilyara/Documents/GitHub/DeepPavlov/download/snips/test.csv file\n",
      "2018-10-31 17:16:00.88 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-10-31 17:16:00.99 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from /home/dilyara/Documents/GitHub/DeepPavlov/download/classifiers/intents_snips_v8/classes.dict]\n",
      "[nltk_data] Downloading package punkt to /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2018-10-31 17:16:00.759 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `/home/dilyara/Documents/GitHub/DeepPavlov/download/embeddings/wiki.en.bin`]\n",
      "Using TensorFlow backend.\n",
      "2018-10-31 17:16:24.732 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-31 17:16:25.213 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from model.h5]\n",
      "2018-10-31 17:16:25.510 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "2018-10-31 17:16:25.512 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9811, \"f1_macro\": 0.9808, \"roc_auc\": 0.9989}, \"time_spent\": \"0:00:02\", \"examples\": [{\"x\": \"Put some mac wiseman in my latino caliente playlist. \", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"AddToPlaylist\"]}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'valid': OrderedDict([('sets_accuracy', 0.9811),\n",
       "              ('f1_macro', 0.9808),\n",
       "              ('roc_auc', 0.9989)])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or one can evaluate model WITHOUT training\n",
    "train_evaluate_model_from_config(config_path, to_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
