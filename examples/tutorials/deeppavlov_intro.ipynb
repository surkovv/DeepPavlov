{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPavlov basics \n",
    "In this tutorial we will construct elementary components needed for working with different NLP tasks. We will go through typical data preprocessing pipeline which will be used in the next tutorials. This part is mostly about low-level elements of the library. In the end will construct a simple bot based on pattern matching and the library abstactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial plan\n",
    "\n",
    "1. [Install the library](#Install-the-library):\n",
    "    - [on Linux](#Install-the-library)\n",
    "    - [on Windows](#Install-the-library-on-Windows-using-Docker)\n",
    "2. [Hello bot](#Hello-bot)\n",
    "3. [Data](#Data):\n",
    "    - [Parsing text data](#Parsing-text-data-into-a-machine-readable-dataset)\n",
    "    - [Preparation of a dictionary](#Prepare-dictionaries)\n",
    "    - [Dataset iterator](#Dataset-Iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "For this task you will need the following libraries:\n",
    " - [Tensorflow](https://www.tensorflow.org) — an open-source software library for Machine Intelligence.\n",
    " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    " - [DeepPavlov](https://github.com/deepmipt/deeppavlov) - open source library for Natural Language Processing\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the library\n",
    "\n",
    "Currently only Linux platform and Python 3.6 is supported\n",
    "\n",
    "- Create a virtual environment with Python 3.6\n",
    "\n",
    "    `virtualenv -p python3.6 env`\n",
    "\n",
    "- Activate the environment.\n",
    "\n",
    "    `source ./env/bin/activate`\n",
    "\n",
    "- Clone the repo and cd to project root\n",
    "\n",
    "    `git clone https://github.com/deepmipt/DeepPavlov.git`\n",
    "    \n",
    "    `cd DeepPavlov`\n",
    "\n",
    "- Install the requirements:\n",
    "\n",
    "    `python setup.py develop`\n",
    "\n",
    "- Install spacy dependencies:\n",
    "\n",
    "    `python -m spacy download en`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the library on Windows using Docker\n",
    "\n",
    "First, install the Docker following these instructions:\n",
    "\n",
    "https://docs.docker.com/docker-for-windows/install\n",
    "\n",
    "Then go to console and get the container with the following command:\n",
    "\n",
    "`docker pull altinsky/convai:deeppavlov`\n",
    "\n",
    "Run the container:\n",
    "\n",
    "`docker run -p 8888:8888 altinsky/convai:deeppavlov`\n",
    "\n",
    "Navigate to http://127.0.0.1:8888/ in your browser.\n",
    "\n",
    "To STOP the container run:\n",
    "\n",
    "`docker stop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will construct a simple bot that relies on pattern matching to perform a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.skills.pattern_matching_skill import PatternMatchingSkill\n",
    "from deeppavlov.core.agent import Agent, HighestConfidenceSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pattern matching skill is the simplest example of Natural Language Understanding component. It will search defined patterns through the text. Let's define some simple patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = PatternMatchingSkill(['Hello world!'], patterns=[\"hi\", \"hello\", \"good day\"])\n",
    "bye = PatternMatchingSkill(['Goodbye world!', 'See you around'],\n",
    "                           patterns=[\"bye\", \"chao\", \"see you\"])\n",
    "fallback = PatternMatchingSkill([\"I don't understand, sorry\", 'I can say \"Hello world!\"'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you provide some patterns to the PatternMatchingSkill it will return confidence = 1 when the skill finds the pattern in given text. If no patterns is provided then confidence 0.5 will be returned in any case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skills are used in the `Agent` which can be treated as a Dialog Manager. The agent must be provided with skills and the selector of skills. A simple skill selector is the HighestConfidenceSelector which will pick the skill with highest confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HelloBot = Agent([hello, bye, fallback], skills_selector=HighestConfidenceSelector())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all processing in the library is performed on batches, we can pass a batch of requests to the bot. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HelloBot(['Hello', 'Bye', 'Or not'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "- create a WhatIsYourName skill\n",
    "- create new agent with this skill\n",
    "- check that all works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = # YOUR_CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Deeppavlov library has functionality to download and decompress the data. For this purpose the `download_decompress` from `data.utils` is used. \n",
    "The following cell will download the CoNLL-2003 data for the Named Entity Recognition (NER) task and put it to the folder `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://lnsigo.mipt.ru/export/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing text data into a machine-readable dataset \n",
    "\n",
    "We will work with a corpus which contains tweets with NE tags. A typical file with NER data contains lines with pairs of tokens (word or punctuation symbol) and tags separated by a whitespace. In many cases additional information such as POS-tags is included. \n",
    "\n",
    "Different documents are separated by lines **started** with **-DOCSTART-** token. Different sentences are separated by an empty line. Example:\n",
    "\n",
    "    -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "\n",
    "    Peter NNP B-NP B-PER\n",
    "    Blackburn NNP I-NP I-PER\n",
    "\n",
    "In this tutorial we will focus only on tokens and tags (first and last elements of the line) and drop POS information located between them.\n",
    "\n",
    "We start by building a class *NerDatasetReader*  that provides functionality for reading the dataset. It returns a dictionary with fields *train*, *test*, and *valid*. Each field stores a list of samples. Each sample is a tuple of tokens and tags. Both tokens and tags are lists. The following example depicts the structure that should be returned by *read* method:\n",
    "\n",
    "    {'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    "     'valid': [...],\n",
    "     'test': [...]}\n",
    "\n",
    "There are three separate parts in the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model.\n",
    " \n",
    "\n",
    "Each of these parts is stored in a separate txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class NerDatasetReader:\n",
    "    def read(self, data_path):\n",
    "        data_parts = ['train', 'valid', 'test']\n",
    "        extension = '.txt'\n",
    "        dataset = {}\n",
    "        for data_part in data_parts:\n",
    "            file_path = Path(data_path) / Path(data_part + extension)\n",
    "            dataset[data_part] = self.read_file(str(file_path))\n",
    "        return dataset\n",
    "            \n",
    "    @staticmethod\n",
    "    def read_file(file_path):\n",
    "        \n",
    "        # Use utf-8 encoding when open the file\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reader = NerDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_reader.read('data/')\n",
    "assert len(dataset) == 3, 'The dataset must be a dict with three fields: train, test, and valid'\n",
    "assert len(set(dataset) & {'train', 'test', 'valid'}) == 3, 'The dataset keys must be exactly train, test, and valid'\n",
    "assert isinstance(dataset['train'][0][0][0], str) and isinstance(dataset['train'][0][0][1], str), 'Both tokens and tags must be strings'\n",
    "assert len(dataset['train']) == 14041, 'there must be exactly 14041 samples in train'\n",
    "assert len(dataset['valid']) == 3250, 'there must be exactly 3250 samples in train'\n",
    "assert len(dataset['test']) == 3453, 'there must be exactly 3453 samples in test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data by running the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sample in dataset['train'][:2]:\n",
    "    for token, tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find an implementation of the dataset reader that implemets the same interfaces in the library: [Conll2003DatasetReader](https://github.com/deepmipt/DeepPavlov/blob/dev/deeppavlov/dataset_readers/conll2003_reader.py). The functionality of the presented code is wider and the `register` wrapper allows to use this component as a part of config file (will be discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Token indices will be used to address a row in embeddings matrix. The mapping for tags will be used to create one-hot ground-truth probability distribution vectors to compute the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the *Vocab* class which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,\n",
    "                 special_tokens=tuple()):\n",
    "        self.special_tokens = special_tokens\n",
    "        self._t2i = defaultdict(lambda: 1)\n",
    "        self._i2t = []\n",
    "        \n",
    "    def fit(self, tokens):\n",
    "        count = 0\n",
    "        self.freqs = Counter(chain(*tokens))\n",
    "        # The first special token will be the default token\n",
    "        for special_token in self.special_tokens:\n",
    "            self._t2i[special_token] = count\n",
    "            self._i2t.append(special_token)\n",
    "            count += 1\n",
    "        for token, freq in self.freqs.most_common():\n",
    "            if token not in self._t2i:\n",
    "                self._t2i[token] = count\n",
    "                self._i2t.append(token)\n",
    "                count += 1\n",
    "\n",
    "    def __call__(self, batch, **kwargs):\n",
    "        # Implement the vocab() method. The input could be a batch of tokens\n",
    "        # or a batch of indices. A batch is a list of utterances where each\n",
    "        # utterance is a list of tokens\n",
    "        pass\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Implement the vocab[] method. The input could be a token\n",
    "        # (string) or an index. You have to detect what type of data\n",
    "        # is key and return. \n",
    "        pass\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._i2t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens\n",
    " - `'O'` for the tag vocab to place out of label tag to the first place with index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "special_tags = ['O']\n",
    "\n",
    "token_vocab = Vocab(special_tokens)\n",
    "tag_vocab = Vocab(special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the vocabularies on the *train* part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_by_sentenses = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentenses = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentenses)\n",
    "tag_vocab.fit(all_tags_by_sentenses)\n",
    "\n",
    "assert len(token_vocab) == 23624, 'There must be exactly 23624 in the token vocab!'\n",
    "assert len(tag_vocab) == 9, 'There must be exactly 9 in the tag vocab!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the indices. Keep in mind that we are working with batches of the following structure:\n",
    "    \n",
    "    [['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_batch = token_vocab([['How', 'to', 'cook', 'a', 'turnip', '?']])\n",
    "\n",
    "assert len(indices_batch) == 1, 'the batch length must be 1'\n",
    "assert isinstance(indices_batch[0][0], int), 'The batch must contain lists of ints!'\n",
    "\n",
    "print(indices_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_indices_batch = tag_vocab([['O', 'O', 'O'], ['B-PER']])\n",
    "\n",
    "assert len(tag_indices_batch) == 2, 'the batch length must be 2'\n",
    "assert isinstance(tag_indices_batch[0][0], int), 'The batch must contain lists of ints!'\n",
    "\n",
    "print(tag_indices_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try converting from indices to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vocab([np.random.randint(0, 512, size=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar vocabulary is already implemented in the [library](https://github.com/deepmipt/DeepPavlov/blob/dev/deeppavlov/core/data/simple_vocab.py). It has extended functionality:\n",
    "- token cutoff by frequency\n",
    "- limitation of the vocabulary size\n",
    "- saving and loading\n",
    "- dict like dunders (\\_\\_contain\\_\\_, \\_\\_len\\_\\_, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iterator\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. You have to iterate over the dataset and generate `x` and `y` batch by batch. The batch of `x`-s is a list of sentences of tokens like\n",
    "\n",
    "    [['Yan', 'is', 'a', 'good', 'fellow],\n",
    "     ['For', 'instance']]\n",
    "\n",
    "and the tag sequence should be:\n",
    "\n",
    "    [['B-PER', 'O', 'O', 'O', 'O'],\n",
    "     ['O', 'O']]\n",
    "\n",
    "An important concept in the batch generation is shuffling. Shuffling is taking sample from the dataset at random order. It is important to train on the shuffled data because large number consequetive samples of the same class may result in pure quality of the model.\n",
    "    \n",
    "The idea behind the iterator is to perform computation in the lazy way. Use yield generator expression to do so. An example of using yield for generator creation is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator():\n",
    "    data = [1, 2, 3]\n",
    "    for d in data:\n",
    "        yield d\n",
    "            \n",
    "print(iterator)\n",
    "    \n",
    "for i in iterator():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the `DatasetIterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, data):\n",
    "        self.data = {\n",
    "            'train': data['train'],\n",
    "            'valid': data['valid'],\n",
    "            'test': data['test']\n",
    "        }\n",
    "\n",
    "    def gen_batches(self, batch_size, data_type='train', shuffle=True):\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterator from the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DatasetIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(data_iterator.gen_batches(2))\n",
    "\n",
    "assert len(x) == 2, 'There must be two examples in the batch!'\n",
    "assert len(y) == 2, 'There must be two examples in the batch!'\n",
    "assert len(x[0]) == len(y[0]), 'The numbers of tokens and tags are different!'\n",
    "assert isinstance(x[0][0], str), 'Token must be a string!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a typical part of the data preprocessing pipeline. This parts will be used in the following tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
