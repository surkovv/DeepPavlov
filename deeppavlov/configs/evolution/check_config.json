{"dataset_reader": {"name": "basic_classification_reader", "x": ["question", "answer"], "y": "label", "data_path": "/home/dilyara.baymurzina/evolution_data/selqa_data"}, "dataset_iterator": {"name": "basic_classification_iterator"}, "chainer": {"in": ["question", "answer"], "in_y": ["y"], "pipe": [{"id": "classes_vocab", "name": "default_vocab", "fit_on": ["y"], "level": "token", "save_path": "/home/dilyara.baymurzina/evolution_data/selqa_data/selqa_classes.dict", "load_path": "/home/dilyara.baymurzina/evolution_data/selqa_data/selqa_classes.dict"}, {"in": ["question"], "out": ["question_lower"], "name": "str_lower"}, {"in": ["answer"], "out": ["answer_lower"], "name": "str_lower"}, {"id": "my_embedder", "name": "fasttext", "save_path": "/home/dilyara.baymurzina/embeddings/wiki.en.bin", "load_path": "/home/dilyara.baymurzina/embeddings/wiki.en.bin", "dim": 300}, {"id": "my_tokenizer", "name": "nltk_tokenizer", "tokenizer": "wordpunct_tokenize"}, {"in": ["question_lower", "answer_lower"], "in_y": ["y"], "out": ["y_labels", "y_probas_dict"], "main": true, "name": "evolution_many_inputs_classification_model", "save_path": "/home/dilyara.baymurzina/evolution_data/selqa_classification/one_neuron_init_part_many_inputs/population_3/evolution_many_inputs_classification_model_9/evolution_many_inputs_classification_model_9", "load_path": "/home/dilyara.baymurzina/evolution_data/selqa_classification/one_neuron_init_part_many_inputs/population_3/evolution_many_inputs_classification_model_9/evolution_many_inputs_classification_model_9", "classes": "#classes_vocab.keys()", "to_evolve": true, "optimizer": "Adam", "loss": "binary_crossentropy", "text_size": [20, 50], "last_layer_activation": "softmax", "model_name": "evolution_many_inputs_classification_model", "embedder": "#my_embedder", "tokenizer": "#my_tokenizer", "n_types": 6, "n_layers": 5, "confident_threshold": 0.4913063945020907, "lear_rate": 0.06101558390361756, "lear_rate_decay": 0.06011880458410778, "0_0_0": {"node_name": "Dense", "node_type": 0, "node_layer": 0, "units": 334, "activation": "sigmoid"}, "0_1_1": {"node_name": "Conv1D", "node_type": 1, "node_layer": 0, "padding": "same", "filters": 70, "kernel_size": 2}, "0_2_2": {"node_name": "CuDNNLSTM", "node_type": 2, "node_layer": 0, "return_sequences": true, "units": 92}, "0_3_3": {"node_name": "BiCuDNNLSTM", "node_type": 3, "node_layer": 0, "return_sequences": true, "units": 280}, "0_4_4": {"node_name": "MaxPooling1D", "node_type": 4, "node_layer": 0, "padding": "same", "pool_size": 5}, "0_5_5": {"node_name": "SelfMultiplicativeAttention", "node_type": 5, "node_layer": 0, "n_hidden": 478, "n_output_features": 184, "activation": "softmax"}, "1_0_6": {"node_name": "Dense", "node_type": 0, "node_layer": 1, "units": 452, "activation": "sigmoid"}, "1_1_7": {"node_name": "Conv1D", "node_type": 1, "node_layer": 1, "padding": "same", "filters": 381, "kernel_size": 4}, "1_2_8": {"node_name": "CuDNNLSTM", "node_type": 2, "node_layer": 1, "return_sequences": true, "units": 203}, "1_3_9": {"node_name": "BiCuDNNLSTM", "node_type": 3, "node_layer": 1, "return_sequences": true, "units": 402}, "1_4_10": {"node_name": "MaxPooling1D", "node_type": 4, "node_layer": 1, "padding": "same", "pool_size": 2}, "1_5_11": {"node_name": "SelfMultiplicativeAttention", "node_type": 5, "node_layer": 1, "n_hidden": 385, "n_output_features": 212, "activation": "sigmoid"}, "2_0_12": {"node_name": "Dense", "node_type": 0, "node_layer": 2, "units": 355, "activation": "relu"}, "2_1_13": {"node_name": "Conv1D", "node_type": 1, "node_layer": 2, "padding": "same", "filters": 413, "kernel_size": 4}, "2_2_14": {"node_name": "CuDNNLSTM", "node_type": 2, "node_layer": 2, "return_sequences": true, "units": 192}, "2_3_15": {"node_name": "BiCuDNNLSTM", "node_type": 3, "node_layer": 2, "return_sequences": true, "units": 427}, "2_4_16": {"node_name": "MaxPooling1D", "node_type": 4, "node_layer": 2, "padding": "same", "pool_size": 4}, "2_5_17": {"node_name": "SelfMultiplicativeAttention", "node_type": 5, "node_layer": 2, "n_hidden": 274, "n_output_features": 465, "activation": "sigmoid"}, "3_0_18": {"node_name": "Dense", "node_type": 0, "node_layer": 3, "units": 489, "activation": "softmax"}, "3_1_19": {"node_name": "Conv1D", "node_type": 1, "node_layer": 3, "padding": "same", "filters": 373, "kernel_size": 4}, "3_2_20": {"node_name": "CuDNNLSTM", "node_type": 2, "node_layer": 3, "return_sequences": true, "units": 463}, "3_3_21": {"node_name": "BiCuDNNLSTM", "node_type": 3, "node_layer": 3, "return_sequences": true, "units": 166}, "3_4_22": {"node_name": "MaxPooling1D", "node_type": 4, "node_layer": 3, "padding": "same", "pool_size": 3}, "3_5_23": {"node_name": "SelfMultiplicativeAttention", "node_type": 5, "node_layer": 3, "n_hidden": 315, "n_output_features": 462, "activation": "sigmoid"}, "4_0_24": {"node_name": "Dense", "node_type": 0, "node_layer": 4, "units": 482, "activation": "softmax"}, "4_1_25": {"node_name": "Conv1D", "node_type": 1, "node_layer": 4, "padding": "same", "filters": 187, "kernel_size": 4}, "4_2_26": {"node_name": "CuDNNLSTM", "node_type": 2, "node_layer": 4, "return_sequences": true, "units": 462}, "4_3_27": {"node_name": "BiCuDNNLSTM", "node_type": 3, "node_layer": 4, "return_sequences": true, "units": 181}, "4_4_28": {"node_name": "MaxPooling1D", "node_type": 4, "node_layer": 4, "padding": "same", "pool_size": 3}, "4_5_29": {"node_name": "SelfMultiplicativeAttention", "node_type": 5, "node_layer": 4, "n_hidden": 469, "n_output_features": 91, "activation": "sigmoid"}, "binary_mask": [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], "nodes": {"0": "0_0_0", "1": "0_1_1", "2": "0_2_2", "3": "0_3_3", "4": "0_4_4", "5": "0_5_5", "6": "1_0_6", "7": "1_1_7", "8": "1_2_8", "9": "1_3_9", "10": "1_4_10", "11": "1_5_11", "12": "2_0_12", "13": "2_1_13", "14": "2_2_14", "15": "2_3_15", "16": "2_4_16", "17": "2_5_17", "18": "3_0_18", "19": "3_1_19", "20": "3_2_20", "21": "3_3_21", "22": "3_4_22", "23": "3_5_23", "24": "4_0_24", "25": "4_1_25", "26": "4_2_26", "27": "4_3_27", "28": "4_4_28", "29": "4_5_29"}}], "out": ["y_labels"]}, "train": {"metric_optimization": "minimize", "metrics": ["classification_log_loss", "classification_accuracy", "classification_f1", "classification_roc_auc"], "validation_patience": 5, "val_every_n_epochs": 5, "log_every_n_epochs": 5, "show_examples": false, "validate_best": true, "test_best": true, "epochs": 77, "batch_size": 51}, "metadata": {"labels": {"telegram_utils": "IntentModel"}}}
